{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16YP15WTzg45UMqTFoqnmGtU9lNH3WxXc",
      "authorship_tag": "ABX9TyPKylDDIWGX/jgq+IoTRKbT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install malaya\n",
        "!pip install PySastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AujcyvN18by8",
        "outputId": "92ec92c9-3300-4f12-862f-d8afd8fa2cbb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: malaya in /usr/local/lib/python3.10/dist-packages (5.0)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (from malaya) (1.2.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from malaya) (6.1.3)\n",
            "Requirement already satisfied: herpetologist in /usr/local/lib/python3.10/dist-packages (from malaya) (0.0.9)\n",
            "Requirement already satisfied: malaya-boilerplate>=0.0.23 in /usr/local/lib/python3.10/dist-packages (from malaya) (0.0.24)\n",
            "Requirement already satisfied: networkx<=2.5.1 in /usr/local/lib/python3.10/dist-packages (from malaya) (2.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from malaya) (1.25.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from malaya) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from malaya) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from malaya) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from malaya) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from malaya) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from malaya) (4.66.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from malaya) (4.38.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from malaya) (1.3.8)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from malaya-boilerplate>=0.0.23->malaya) (0.20.3)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from networkx<=2.5.1->malaya) (4.4.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser->malaya) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser->malaya) (2023.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->malaya) (5.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->malaya) (0.2.13)\n",
            "Requirement already satisfied: memoization in /usr/local/lib/python3.10/dist-packages (from herpetologist->malaya) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->malaya) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->malaya) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->malaya) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->malaya) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->malaya) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->malaya) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->malaya) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->malaya) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->malaya) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->malaya) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->malaya) (0.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->malaya-boilerplate>=0.0.23->malaya) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->malaya-boilerplate>=0.0.23->malaya) (4.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser->malaya) (1.16.0)\n",
            "Collecting PySastrawi\n",
            "  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PySastrawi\n",
            "Successfully installed PySastrawi-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "\n",
        "import numpy as np\n",
        "import malaya\n",
        "from malaya.tokenizer import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "sastrawi = malaya.stem.sastrawi()\n",
        "\n",
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    Split sentence into array of tokens.\n",
        "    A token can be a word or punctuation character, or number.\n",
        "    \"\"\"\n",
        "\n",
        "    return tokenizer.tokenize(sentence)\n",
        "\n",
        "\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    Stemming is a process to find the root form of the word.\n",
        "\n",
        "    Example:\n",
        "    words = [\"menyeru\", \"menyerukanlah\"]\n",
        "    words = [stem(w) for w in words]\n",
        "    -> [\"seru\"]\n",
        "    \"\"\"\n",
        "\n",
        "    return sastrawi.stem(word.lower())\n",
        "\n",
        "\n",
        "def bag_of_words(tokenized_sentence, words):\n",
        "    \"\"\"\n",
        "    Return bag of words array:\n",
        "    1 for each known word that exists in the sentence, 0 otherwise.\n",
        "\n",
        "    Example:\n",
        "    sentence = [\"apa\", \"khabar\"]\n",
        "    words = [\"hello\", \"apa\", \"khabar\"]\n",
        "    bow = [0, 1, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    # Stem each word\n",
        "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
        "    # Initialize bag with 0 for each word\n",
        "    bag = np.zeros(len(words), dtype=np.float32)\n",
        "    for idx, w in enumerate(words):\n",
        "        if w in sentence_words:\n",
        "            bag[idx] = 1\n",
        "\n",
        "    return bag"
      ],
      "metadata": {
        "id": "rSnWTQLpdLPm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        # No activation and softmax at the end\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "gLgMerSqlmoP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "with open(\"intents.json\", \"r\") as f:\n",
        "  intents = json.load(f)\n",
        "\n",
        "  all_words = []\n",
        "  tags = []\n",
        "  xy = []\n",
        "\n",
        "  for intent in intents[\"intents\"]:\n",
        "    tag = intent[\"tag\"]\n",
        "    tags.append(tag)\n",
        "\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "      word = tokenize(pattern)\n",
        "      all_words.extend(word)\n",
        "      xy.append((word, tag))\n",
        "\n",
        "  ignore_words = [\"?\", \".\", \"!\"]\n",
        "  all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "  tags = sorted(set(tags))\n",
        "\n",
        "  print(f\"patterns: {len(xy)}\", xy)\n",
        "  print(f\"tags: {len(tags)}\", tags)\n",
        "  print(f\"unique stemmed words: {len(all_words)}\", all_words)\n",
        "\n",
        "  # Create training data\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "\n",
        "  for (pattern, tag) in xy:\n",
        "    bag = bag_of_words(pattern, all_words)\n",
        "    x_train.append(bag)\n",
        "\n",
        "    label = tags.index(tag)\n",
        "    y_train.append(label)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  y_train = np.array(y_train)\n",
        "\n",
        "  # Hyperparameters\n",
        "  # See: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#hyperparameters\n",
        "  num_epochs = 1000\n",
        "  batch_size = 8\n",
        "  learning_rate = 0.001\n",
        "  input_size = len(x_train[0])\n",
        "  hidden_size = 8\n",
        "  output_size = len(tags)\n",
        "\n",
        "  print(f\"input size: {input_size}, output size: {output_size}\")\n",
        "\n",
        "  class ChatDataset(Dataset):\n",
        "    def __init__(self):\n",
        "      self.n_samples = len(x_train)\n",
        "      self.x_data = x_train\n",
        "      self.y_data = y_train\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "  dataset = ChatDataset()\n",
        "  train_loader = DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=0\n",
        "  )\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "  # Loss and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "      words = words.to(device)\n",
        "      labels = labels.to(dtype=torch.long).to(device)\n",
        "      outputs = model(words)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "      print (f\"epoch {epoch+1}/{num_epochs}, loss: {loss.item():.8f}\")\n",
        "\n",
        "  print(f\"final loss: {loss.item():.8f}\")\n",
        "\n",
        "  torch.save({\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"input_size\": input_size,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"output_size\": output_size,\n",
        "    \"all_words\": all_words,\n",
        "    \"tags\": tags\n",
        "  }, \"data.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1xVB5JZcNR0",
        "outputId": "319b818e-cc65-4538-fd9c-cf9cb99bdd35"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patterns: 26 [(['hi'], 'greeting'), (['hai'], 'greeting'), (['hello'], 'greeting'), (['assalamualaikum'], 'greeting2'), (['salam'], 'greeting2'), (['slm'], 'greeting2'), (['mat', 'pagi'], 'greeting_time_morning'), (['selamat', 'pagi'], 'greeting_time_morning'), (['pagi'], 'greeting_time_morning'), (['mat', 'malam'], 'greeting_time_night'), (['selamat', 'malam'], 'greeting_time_night'), (['malam'], 'greeting_time_night'), (['siapa', 'awak', '?'], 'name'), (['siapa', 'kau', '?'], 'name'), (['kau', 'ni', 'siapa', '?'], 'name'), (['nama', 'apa', '?'], 'name'), (['apa', 'nama', 'kau', '?'], 'name'), (['single', 'tak', '?'], 'relationship_status'), (['taken', 'ke', '?'], 'relationship_status'), (['single', 'or', 'taken'], 'relationship_status'), (['taken', '?'], 'relationship_status'), (['single', '?'], 'relationship_status'), (['apa', 'status', 'relay', 'kau', '?'], 'relationship_status'), (['ada', 'calon', 'tak', '?'], 'relationship'), (['ada', 'gf', '?'], 'relationship'), (['ada', 'girlfriend', '?'], 'relationship')]\n",
            "tags: 7 ['greeting', 'greeting2', 'greeting_time_morning', 'greeting_time_night', 'name', 'relationship', 'relationship_status']\n",
            "unique stemmed words: 48 ['hi', 'hai', 'hello', 'assalamualaikum', 'salam', 'slm', 'mat', 'pagi', 'selamat', 'pagi', 'pagi', 'mat', 'malam', 'selamat', 'malam', 'malam', 'siapa', 'awak', 'siapa', 'kau', 'kau', 'ni', 'siapa', 'nama', 'apa', 'apa', 'nama', 'kau', 'single', 'tak', 'taken', 'ke', 'single', 'or', 'taken', 'taken', 'single', 'apa', 'status', 'relay', 'kau', 'ada', 'calon', 'tak', 'ada', 'gf', 'ada', 'girlfriend']\n",
            "input size: 48, output size: 7\n",
            "epoch 100/1000, loss: 1.58224869\n",
            "epoch 200/1000, loss: 0.05472569\n",
            "epoch 300/1000, loss: 0.01882987\n",
            "epoch 400/1000, loss: 0.00403202\n",
            "epoch 500/1000, loss: 0.00077354\n",
            "epoch 600/1000, loss: 0.00965730\n",
            "epoch 700/1000, loss: 0.00151406\n",
            "epoch 800/1000, loss: 0.00176064\n",
            "epoch 900/1000, loss: 0.00187542\n",
            "epoch 1000/1000, loss: 0.00179861\n",
            "final loss: 0.00179861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYhxaGhVINlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat\n",
        "\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open(\"intents.json\", \"r\") as json_data:\n",
        "  intents = json.load(json_data)\n",
        "  data = torch.load(\"data.pth\")\n",
        "\n",
        "  input_size = data[\"input_size\"]\n",
        "  hidden_size = data[\"hidden_size\"]\n",
        "  output_size = data[\"output_size\"]\n",
        "  all_words = data[\"all_words\"]\n",
        "  tags = data[\"tags\"]\n",
        "  model_state = data[\"model_state\"]\n",
        "\n",
        "  model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "  model.load_state_dict(model_state)\n",
        "  model.eval()\n",
        "\n",
        "  while True:\n",
        "      sentence = input(\"You: \")\n",
        "      if sentence == \"quit\":\n",
        "          break\n",
        "\n",
        "      sentence = tokenize(sentence)\n",
        "      x = bag_of_words(sentence, all_words)\n",
        "      x = x.reshape(1, x.shape[0])\n",
        "      x = torch.from_numpy(x).to(device)\n",
        "\n",
        "      output = model(x)\n",
        "      _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "      tag = tags[predicted.item()]\n",
        "      probs = torch.softmax(output, dim=1)\n",
        "      prob = probs[0][predicted.item()]\n",
        "      if prob.item() > 0.75:\n",
        "          for intent in intents[\"intents\"]:\n",
        "              if tag == intent[\"tag\"]:\n",
        "                  print(f\"Bot: {random.choice(intent['responses'])}\")\n",
        "      else:\n",
        "          print(\"Bot: tak paham\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "-YNtKG2RlRyp",
        "outputId": "af364a42-6097-46d5-ecbb-570b084ec30b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hai awak\n",
            "Bot: hello awaks\n",
            "You: single or taken\n",
            "Bot: masih single\n",
            "You: mat pagi\n",
            "Bot: morning wak\n",
            "You: ada gf tak?\n",
            "Bot: tak sebab tak hesmes :(\n",
            "You: apakah status relay anda?\n",
            "Bot: single abadi\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-48dce73e39a1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}